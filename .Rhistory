count(word, sort = TRUE)
# combine tokenized books
library(tidyr)
frequency <- bind_rows(mutate(tidy_bronte, author = "Bront? Sisters"),
mutate(tidy_hgwells, author = "H.G. Wells"),
mutate(tidy_books, author = "Jane Austen")) %>%
# str_extract() used b/c the UTF-8 encoded texts from Project Gutenberg have
# some words with underscores around them to indicate emphasis. The tokenizer
# treated these words differently.
mutate(word = str_extract(word, "[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion) %>%
gather(author, proportion, `Bront? Sisters`:`H.G. Wells`)
# visusalize
library(scales)
# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`
, color=abs(`Jane Austen` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits=c(0, 0.001), low="darkslategray4", high="gray75") +
facet_wrap(~author, ncol = 2) +
theme(legend.position="none") +
labs(y = "Jane Austen", x = NULL)
# correlation of word counts (Bronte vs. Jane Austen)
cor.test(data = frequency[frequency$author == "Bront? Sisters",],
~ proportion + `Jane Austen`)
# correlation of word counts (HG Wells vs. Jane Austen)
cor.test(data = frequency[frequency$author == "H.G. Wells",],
~ proportion + `Jane Austen`)
################################################################################
# Sentiment Analysis
# https://github.com/dgrtwo/tidy-text-mining/blob/master/02-sentiment-analysis.Rmd
################################################################################
library(tidytext)
sentiments
# get lexicons
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
install.packages("textdata")
library(textdata)
# get lexicons
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
# load Austen books text data, add linenumber & chapter, then tokenize
library(janeaustenr)
library(dplyr)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
# get joy scores from nrc lexicon only
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%                # take tidy_books dataset
filter(book == "Emma") %>%  # filter down to Emma only
inner_join(nrc_joy) %>%     # only inculde nrc joy words
count(word, sort = TRUE)    # sort from highest word count descending
# sentiment changes throughout each novel
library(tidyr)
jane_austen_sentiment <- tidy_books %>%    # set tidy_books as jane_austen_sent
inner_join(get_sentiments("bing")) %>%   # add bing sentiments and filter to only those
count(book, index = linenumber %/% 80, sentiment) %>%  # create sections of 80 lines
spread(sentiment, n, fill = 0) %>%       # add both negative and positive sentiments
mutate(sentiment = positive - negative)  # calcuate net sentiment
# plot sentiment
library(ggplot2)
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
# Comparing the three sentiment dictionaries
# First lets just look at the Pride and Prejudice text
pride_prejudice <- tidy_books %>%
filter(book == "Pride & Prejudice")
pride_prejudice
# calculate the AFINN sentiment for chucks of 80 lines of text
afinn <- pride_prejudice %>%               # load pride book
inner_join(get_sentiments("afinn")) %>%  # add sentiment scores for words
group_by(index = linenumber %/% 80) %>%  # add index so every 80 lines are group
summarise(sentiment = sum(score)) %>%    # summarize over index
mutate(method = "AFINN")                 # add a "method" column with AFINN
# calculate Bing and NRC sentiments
bing_and_nrc <- bind_rows(pride_prejudice %>%
inner_join(get_sentiments("bing")) %>%
mutate(method = "Bing et al."), pride_prejudice %>%
inner_join(get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative"))) %>%
mutate(method = "NRC")) %>%
count(method, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
# combine 3 different sentiment scores and plot them
bind_rows(afinn,
bing_and_nrc) %>%
ggplot(aes(index, sentiment, fill = method)) +
geom_col(show.legend = FALSE) +
facet_wrap(~method, ncol = 1, scales = "free_y")
# Count of NRC positive and negative sentiments
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative")) %>%
count(sentiment)
# Count of Bing positive and negative sentiments
get_sentiments("bing") %>%
count(sentiment)
# Most common positive and negative words
bing_word_counts <- tidy_books %>%        # set tidy_books as bing_word_counts
inner_join(get_sentiments("bing")) %>%  # obtain Bing sentiments only
count(word, sentiment, sort = TRUE) %>% # count the word frequency
ungroup()
bing_word_counts
# Plot most common positive and negative words
bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
# create a custom stop words list using the stop_words list and by adding
# any additional words we want (e.g. miss)
custom_stop_words <- bind_rows(data_frame(word = c("miss"),
lexicon = c("custom")),
stop_words)
custom_stop_words
# create a word cloud
library(wordcloud)
tidy_books %>%                               # take tidy_books dataset
anti_join(stop_words) %>%                  # remove stop words
count(word) %>%                            # count frequency of words
with(wordcloud(word, n, max.words = 100))  # create a word cloud w/ <= 100 words
################################################################################
# Text Analytics
#
# Matthew A. Lanham
# https://github.com/dgrtwo/tidy-text-mining/blob/master/01-tidy-text.Rmd
################################################################################
# Example: Text data.frame to tibble data.frame
text <- c("Because I could not stop for Death -",
"He kindly stopped for me -",
"The Carriage held but just Ourselves -",
"and Immortality")
text
library(dplyr)
text_df <- data_frame(line=1:4, text=text)
text_df
# clean up
#rm(text_df, text)
# Split a column into tokens using the tokenizers package, splitting the table
# into one-token-per-row.
library(tidytext)
text_df %>%
unnest_tokens(output=word, input=text)
# Example: Jane Austen's Published Novels
library(janeaustenr)
library(dplyr)
library(stringr)
# raw books text data
original_books <- austen_books()
class(original_books)
# add linenumber and chapter columns
original_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup()
# look at first few records of tibble
original_books
# tokenize the data
library(tidytext)
tidy_books <- original_books %>%
unnest_tokens(output=word, input=text)
# look at first few records of tidy_books tibble
tidy_books
# load the stop_words dictionary; note that you can always add words or remove
# words to this if you like based on your own problems context/domain
data(stop_words)
head(stop_words)
table(stop_words$lexicon) # 3 different lexicon's
# remove stop words from dataset
tidy_books <- tidy_books %>%
anti_join(stop_words)
# most common words
tidy_books %>%
count(word, sort = TRUE)
# visualize words
library(ggplot2)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
# pull book data from the gutenberg project
library(gutenbergr)
hgwells <- gutenberg_download(c(35, 36, 5230, 159))
#load("data/hgwells.rda") #not needed on RStudio Server
tidy_hgwells <- hgwells %>%
unnest_tokens(word, text) %>%   # tokenize
anti_join(stop_words)           # remove stop words
# most common words
tidy_hgwells %>%
count(word, sort = TRUE)
# pull book data from the gutenberg project
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
#load("data/bronte.rda")    #not needed on RStudio Server
tidy_bronte <- bronte %>%
unnest_tokens(word, text) %>%   # tokenize
anti_join(stop_words)           # remove stop words
# most common words
tidy_bronte %>%
count(word, sort = TRUE)
# combine tokenized books
library(tidyr)
frequency <- bind_rows(mutate(tidy_bronte, author = "Bront? Sisters"),
mutate(tidy_hgwells, author = "H.G. Wells"),
mutate(tidy_books, author = "Jane Austen")) %>%
# str_extract() used b/c the UTF-8 encoded texts from Project Gutenberg have
# some words with underscores around them to indicate emphasis. The tokenizer
# treated these words differently.
mutate(word = str_extract(word, "[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion) %>%
gather(author, proportion, `Bront? Sisters`:`H.G. Wells`)
# visusalize
library(scales)
# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`
, color=abs(`Jane Austen` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits=c(0, 0.001), low="darkslategray4", high="gray75") +
facet_wrap(~author, ncol = 2) +
theme(legend.position="none") +
labs(y = "Jane Austen", x = NULL)
# correlation of word counts (Bronte vs. Jane Austen)
cor.test(data = frequency[frequency$author == "Bront? Sisters",],
~ proportion + `Jane Austen`)
# correlation of word counts (HG Wells vs. Jane Austen)
cor.test(data = frequency[frequency$author == "H.G. Wells",],
~ proportion + `Jane Austen`)
################################################################################
# Sentiment Analysis
# https://github.com/dgrtwo/tidy-text-mining/blob/master/02-sentiment-analysis.Rmd
################################################################################
library(tidytext)
sentiments
# get lexicons
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
# load Austen books text data, add linenumber & chapter, then tokenize
library(janeaustenr)
library(dplyr)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
# get joy scores from nrc lexicon only
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%                # take tidy_books dataset
filter(book == "Emma") %>%  # filter down to Emma only
inner_join(nrc_joy) %>%     # only inculde nrc joy words
count(word, sort = TRUE)    # sort from highest word count descending
# sentiment changes throughout each novel
library(tidyr)
jane_austen_sentiment <- tidy_books %>%    # set tidy_books as jane_austen_sent
inner_join(get_sentiments("bing")) %>%   # add bing sentiments and filter to only those
count(book, index = linenumber %/% 80, sentiment) %>%  # create sections of 80 lines
spread(sentiment, n, fill = 0) %>%       # add both negative and positive sentiments
mutate(sentiment = positive - negative)  # calcuate net sentiment
# plot sentiment
library(ggplot2)
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
# Comparing the three sentiment dictionaries
# First lets just look at the Pride and Prejudice text
pride_prejudice <- tidy_books %>%
filter(book == "Pride & Prejudice")
pride_prejudice
# calculate the AFINN sentiment for chucks of 80 lines of text
afinn <- pride_prejudice %>%               # load pride book
inner_join(get_sentiments("afinn")) %>%  # add sentiment scores for words
group_by(index = linenumber %/% 80) %>%  # add index so every 80 lines are group
summarise(sentiment = sum(score)) %>%    # summarize over index
mutate(method = "AFINN")                 # add a "method" column with AFINN
# calculate Bing and NRC sentiments
bing_and_nrc <- bind_rows(pride_prejudice %>%
inner_join(get_sentiments("bing")) %>%
mutate(method = "Bing et al."), pride_prejudice %>%
inner_join(get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative"))) %>%
mutate(method = "NRC")) %>%
count(method, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
# combine 3 different sentiment scores and plot them
bind_rows(afinn,
bing_and_nrc) %>%
ggplot(aes(index, sentiment, fill = method)) +
geom_col(show.legend = FALSE) +
facet_wrap(~method, ncol = 1, scales = "free_y")
# Count of NRC positive and negative sentiments
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative")) %>%
count(sentiment)
# Count of Bing positive and negative sentiments
get_sentiments("bing") %>%
count(sentiment)
# Most common positive and negative words
bing_word_counts <- tidy_books %>%        # set tidy_books as bing_word_counts
inner_join(get_sentiments("bing")) %>%  # obtain Bing sentiments only
count(word, sentiment, sort = TRUE) %>% # count the word frequency
ungroup()
bing_word_counts
# Plot most common positive and negative words
bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
# create a custom stop words list using the stop_words list and by adding
# any additional words we want (e.g. miss)
custom_stop_words <- bind_rows(data_frame(word = c("miss"),
lexicon = c("custom")),
stop_words)
custom_stop_words
# create a word cloud
library(wordcloud)
tidy_books %>%                               # take tidy_books dataset
anti_join(stop_words) %>%                  # remove stop words
count(word) %>%                            # count frequency of words
with(wordcloud(word, n, max.words = 100))  # create a word cloud w/ <= 100 words
# look at first few records of tibble
original_books
tidy_books <- original_books %>%
unnest_tokens(output=word, input=text)
# look at first few records of tidy_books tibble
tidy_books
# load the stop_words dictionary; note that you can always add words or remove
# words to this if you like based on your own problems context/domain
data(stop_words)
head(stop_words)
table(stop_words$lexicon) # 3 different lexicon's
# remove stop words from dataset
tidy_books <- tidy_books %>%
anti_join(stop_words)
summarise(tidy_books)
View(tidy_books)
# remove stop words from dataset
tidy_books <- tidy_books %>%
anti_join(stop_words)
summarise(tidy_books)
# most common words
tidy_books %>%
count(word, sort = TRUE)
# visualize words
library(ggplot2)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
# pull book data from the gutenberg project
library(gutenbergr)
hgwells <- gutenberg_download(c(35, 36, 5230, 159))
#load("data/hgwells.rda") #not needed on RStudio Server
tidy_hgwells <- hgwells %>%
unnest_tokens(word, text) %>%   # tokenize
anti_join(stop_words)           # remove stop words
# most common words
tidy_hgwells %>%
count(word, sort = TRUE)
# pull book data from the gutenberg project
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
#load("data/bronte.rda")    #not needed on RStudio Server
tidy_bronte <- bronte %>%
unnest_tokens(word, text) %>%   # tokenize
anti_join(stop_words)           # remove stop words
# most common words
tidy_bronte %>%
count(word, sort = TRUE)
# combine tokenized books
library(tidyr)
frequency <- bind_rows(mutate(tidy_bronte, author = "Bront? Sisters"),
mutate(tidy_hgwells, author = "H.G. Wells"),
mutate(tidy_books, author = "Jane Austen")) %>%
# str_extract() used b/c the UTF-8 encoded texts from Project Gutenberg have
# some words with underscores around them to indicate emphasis. The tokenizer
# treated these words differently.
mutate(word = str_extract(word, "[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion) %>%
gather(author, proportion, `Bront? Sisters`:`H.G. Wells`)
# visusalize
library(scales)
# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`
, color=abs(`Jane Austen` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits=c(0, 0.001), low="darkslategray4", high="gray75") +
facet_wrap(~author, ncol = 2) +
theme(legend.position="none") +
labs(y = "Jane Austen", x = NULL)
# correlation of word counts (Bronte vs. Jane Austen)
cor.test(data = frequency[frequency$author == "Bront? Sisters",],
~ proportion + `Jane Austen`)
# correlation of word counts (HG Wells vs. Jane Austen)
cor.test(data = frequency[frequency$author == "H.G. Wells",],
~ proportion + `Jane Austen`)
################################################################################
# Sentiment Analysis
# https://github.com/dgrtwo/tidy-text-mining/blob/master/02-sentiment-analysis.Rmd
################################################################################
library(tidytext)
sentiments
# get lexicons
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
# load Austen books text data, add linenumber & chapter, then tokenize
library(janeaustenr)
library(dplyr)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
# get joy scores from nrc lexicon only
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%                # take tidy_books dataset
filter(book == "Emma") %>%  # filter down to Emma only
inner_join(nrc_joy) %>%     # only inculde nrc joy words
count(word, sort = TRUE)    # sort from highest word count descending
# sentiment changes throughout each novel
library(tidyr)
jane_austen_sentiment <- tidy_books %>%    # set tidy_books as jane_austen_sent
inner_join(get_sentiments("bing")) %>%   # add bing sentiments and filter to only those
count(book, index = linenumber %/% 80, sentiment) %>%  # create sections of 80 lines
spread(sentiment, n, fill = 0) %>%       # add both negative and positive sentiments
mutate(sentiment = positive - negative)  # calcuate net sentiment
# plot sentiment
library(ggplot2)
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
# Comparing the three sentiment dictionaries
# First lets just look at the Pride and Prejudice text
pride_prejudice <- tidy_books %>%
filter(book == "Pride & Prejudice")
pride_prejudice
# calculate the AFINN sentiment for chucks of 80 lines of text
afinn <- pride_prejudice %>%               # load pride book
inner_join(get_sentiments("afinn")) %>%  # add sentiment scores for words
group_by(index = linenumber %/% 80) %>%  # add index so every 80 lines are group
summarise(sentiment = sum(score)) %>%    # summarize over index
mutate(method = "AFINN")                 # add a "method" column with AFINN
?glm
